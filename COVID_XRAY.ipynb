{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVID_XRAY.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f1e92b422044b319ce1b944bde861c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d272a1de65f84bc398e00699d8824403",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3627b146161b41b6b4c770926debfabe",
              "IPY_MODEL_b4e8c8325e034316964bd12d503779d6"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "d272a1de65f84bc398e00699d8824403": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "3627b146161b41b6b4c770926debfabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_de652443177e4596844ed016ef99acda",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244418560,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244418560,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d9c795458cf4aac9c1fac3b53348d7b"
          },
          "model_module_version": "1.5.0"
        },
        "b4e8c8325e034316964bd12d503779d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9687722ebe2a4a09a9d66eaa84577d3f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 233M/233M [00:15&lt;00:00, 15.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0228361d299c4fab84fc90222041bff6"
          },
          "model_module_version": "1.5.0"
        },
        "de652443177e4596844ed016ef99acda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "3d9c795458cf4aac9c1fac3b53348d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "9687722ebe2a4a09a9d66eaa84577d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "0228361d299c4fab84fc90222041bff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mz-zarei/COVID-XRAY/blob/main/COVID_XRAY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-DFlMp3G9lN"
      },
      "source": [
        "## **1-Importing Libraries and Unzipping Data** adjust the address to the submitted zip file (DL1_MZarei.rar) and zipped data (DataChallenge1.rar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9mywcOfa-jb"
      },
      "source": [
        "import os\n",
        "os.mkdir(\"MZareiFolder\") \n",
        "os.mkdir(\"MZareiFolder/MyData\") \n",
        "os.mkdir(\"MZareiFolder/MyData/Class1\") \n",
        "os.mkdir(\"MZareiFolder/MyData/Class0\") \n",
        "os.makedirs('MZareiFolder/MyData/splited')\n",
        "os.makedirs('MZareiFolder/MyData/splited/train')\n",
        "os.makedirs('MZareiFolder/MyData/splited/val')\n",
        "\n",
        "# Unzipping the submitted RAR file including data and models\n",
        "!unrar x \"/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/DL1_MZarei.rar\" \"MZareiFolder\"\n",
        "\n",
        "# Adjust the path to the zipped data file\n",
        "!unrar x \"/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/DataChallenge1.rar\" \"MZareiFolder\"\n",
        "\n",
        "# Adjust the path where the model should be saved\n",
        "save_model_path = \"MZareiFolder/\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER0U4b4rLdzc"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import cv2\n",
        "import os, shutil\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from PIL import *\n",
        "from collections import OrderedDict\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score \n",
        "import random\n",
        "from collections import Counter \n",
        "\n",
        "\n",
        "\n",
        "## Tried to Fix all seeds but unsuccessful!\n",
        "\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "torch.backends.cudnn.enabled = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toHbQ-b7VJlO"
      },
      "source": [
        "!pip install split_folders\n",
        "import splitfolders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpFQnetjbIoJ"
      },
      "source": [
        "## **2-Data Preperation** Including Validation/Train split and creating DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy8kYgiB7eSg",
        "outputId": "978aaba8-f0d5-42ff-8ee1-ca1136ddfc83"
      },
      "source": [
        "## Arrangeing DataSet so that pytorch ImageFolder can be used\n",
        "\n",
        "imagePaths = list(paths.list_images(\"MZareiFolder/train/train\"))\n",
        "train_labels = pd.read_csv(\"MZareiFolder/train_labels.csv\")\n",
        "\n",
        "\n",
        "\n",
        "for imagePath in tqdm(imagePaths):\n",
        "\n",
        "    image_name = imagePath.split(os.path.sep)[-1]\n",
        "    # find image lable in train_labels\n",
        "    image_label = train_labels.loc[train_labels['File'] == image_name, 'Label'].iloc[0]\n",
        "    if image_label == 1:\n",
        "        shutil.copy(imagePath, \"MZareiFolder/MyData/Class1\" + \"/\" + image_name)\n",
        "    if image_label == 0:\n",
        "        shutil.copy(imagePath, \"MZareiFolder/MyData/Class0\" + \"/\" + image_name)\n",
        "\n",
        "print(len(list(paths.list_images(\"MZareiFolder/MyData/Class0\"))))\n",
        "print(len(list(paths.list_images(\"MZareiFolder/MyData/Class1\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15264/15264 [00:19<00:00, 769.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13694\n",
            "1570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chFM7MRK_AOV",
        "outputId": "c6d8fbe9-48e7-49e3-ac53-cd067ee02c68"
      },
      "source": [
        "# Split Data into train/Validation\n",
        "\n",
        "splitfolders.ratio(\"MZareiFolder/MyData\", output='MZareiFolder/MyData/splited', seed=1, ratio=(0.90, 0.1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying files: 15264 files [00:02, 7027.56 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmfbv8-MU0BG",
        "outputId": "3b46ec48-5fed-4692-f5e6-02c9b0c0254b"
      },
      "source": [
        "print('train data:')\n",
        "print(len(list(paths.list_images(\"MZareiFolder/MyData/splited/train/Class0\"))))\n",
        "print(len(list(paths.list_images(\"MZareiFolder/MyData/splited/train/Class1\"))))\n",
        "\n",
        "print('Validation data:')\n",
        "print(len(list(paths.list_images(\"MZareiFolder/MyData/splited/val/Class0\"))))\n",
        "print(len(list(paths.list_images(\"MZareiFolder/MyData/splited/val/Class1\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data:\n",
            "12324\n",
            "1413\n",
            "Validation data:\n",
            "1370\n",
            "157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzmDBqcsbDvo"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # transforms.RandomRotation((-5,5)),\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "TrainData = datasets.ImageFolder(root=\"MZareiFolder/MyData/splited/train\", transform=transform_train)\n",
        "ValidationData = datasets.ImageFolder(root=\"MZareiFolder/MyData/splited/val\", transform=transform_val)\n",
        "\n",
        "\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler([1,8], num_samples=2, replacement=True)\n",
        "\n",
        "train_loader=torch.utils.data.DataLoader(TrainData,batch_size=8, shuffle=sampler)\n",
        "val_loader=torch.utils.data.DataLoader(ValidationData, batch_size=32, shuffle=False)\n",
        "\n",
        "# print(\"Val batch number: \", len(list(enumerate(val_loader))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4eN1LcFi_pa"
      },
      "source": [
        "## **3- Defining Train and Evaluation Functions and Mixup data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9HJn4_abzrS"
      },
      "source": [
        "def mixup_data(x, y, alpha=0.5, use_cuda=True):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, dataloader):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001, eps = 0.0001/epochs)\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    total_steps = 0\n",
        "    for i, (images, labels) in tqdm(enumerate(dataloader)):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        inputs, targets_a, targets_b, lam = mixup_data(images, labels, 0.5, True)\n",
        "        inputs, targets_a, targets_b = map(Variable, (inputs, targets_a, targets_b))\n",
        "\n",
        "\n",
        "\n",
        "        output = model(inputs)\n",
        "        # loss = F.cross_entropy(output, labels, weight=torch.FloatTensor([1,2]).to(device))\n",
        "        # loss = F.cross_entropy(output, labels)\n",
        "        loss = mixup_criterion(criterion, output, targets_a, targets_b, lam)\n",
        "        #This is where the model learns by backpropagating\n",
        "        loss.backward()\n",
        "        \n",
        "        #And optimizes its weights here\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        total_steps += 1\n",
        "    \n",
        "    return running_loss/total_steps\n",
        "    \n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    test_loss = 0\n",
        "    pred_list = np.array([])\n",
        "    label_list = np.array([])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in tqdm(enumerate(dataloader)):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            output = model(images)\n",
        "            test_loss += F.cross_entropy(output, labels, reduction = 'sum').item()\n",
        "\n",
        "            prd = output.argmax(1)\n",
        "            label_list= np.append(label_list, labels.cpu().numpy())\n",
        "            pred_list = np.append(pred_list, prd.cpu().numpy())\n",
        "\n",
        "            correct += torch.sum(labels == prd)\n",
        "    \n",
        "    test_loss /= len(dataloader.dataset)\n",
        "    accuracy = accuracy_score(label_list, pred_list)\n",
        "    f1 = f1_score(label_list, pred_list)\n",
        "    recall = recall_score(label_list, pred_list)\n",
        "    precision = precision_score(label_list, pred_list)\n",
        "    return test_loss, accuracy, f1, recall, precision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiSk2gvUjGWr"
      },
      "source": [
        "## **4-MODEL ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv4c3oj9b26I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600,
          "referenced_widgets": [
            "2f1e92b422044b319ce1b944bde861c5",
            "d272a1de65f84bc398e00699d8824403",
            "3627b146161b41b6b4c770926debfabe",
            "b4e8c8325e034316964bd12d503779d6",
            "de652443177e4596844ed016ef99acda",
            "3d9c795458cf4aac9c1fac3b53348d7b",
            "9687722ebe2a4a09a9d66eaa84577d3f",
            "0228361d299c4fab84fc90222041bff6"
          ]
        },
        "outputId": "a2aba117-cdf4-4d78-d92b-41d146cc28fa"
      },
      "source": [
        "# model = models.googlenet(True)\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "# ## Re-designing the classifier head\n",
        "# classifier = nn.Sequential(OrderedDict([    \n",
        "#                         ('fc1', nn.Linear(1024, 256)),\n",
        "#                         ('relu', nn.ReLU()), \n",
        "#                         ('dropout1', nn.Dropout(0.3)),\n",
        "#                         ('fc2', nn.Linear(256, 128)),\n",
        "#                         ('dropout2', nn.Dropout(0.3)),\n",
        "#                         ('relu', nn.ReLU()),\n",
        "#                         ('fc3', nn.Linear(128, 2)),\n",
        "#                         ('output', nn.LogSoftmax())\n",
        "#                         ]))\n",
        "\n",
        "# model.fc = classifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Loading AlexNet weights\n",
        "model = models.alexnet(True)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "## Re-designing the classifier head\n",
        "classifier = nn.Sequential(OrderedDict([    \n",
        "                        ('flat', nn.Flatten()),  \n",
        "                        ('dropout1', nn.Dropout(0.3)),          \n",
        "                        ('fc1', nn.Linear(9216, 256)),\n",
        "                        ('relu', nn.ReLU()), \n",
        "                        ('dropout1', nn.Dropout(0.3)),\n",
        "                        ('fc2', nn.Linear(256, 128)),\n",
        "                        ('dropout2', nn.Dropout(0.3)),\n",
        "                        ('relu', nn.ReLU()),\n",
        "                        ('fc3', nn.Linear(128, 2)),\n",
        "                        ('output', nn.LogSoftmax())\n",
        "                        ]))\n",
        "\n",
        "model.classifier = classifier\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f1e92b422044b319ce1b944bde861c5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=244418560.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (flat): Flatten(start_dim=1, end_dim=-1)\n",
            "    (dropout1): Dropout(p=0.3, inplace=False)\n",
            "    (fc1): Linear(in_features=9216, out_features=256, bias=True)\n",
            "    (relu): ReLU()\n",
            "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (dropout2): Dropout(p=0.3, inplace=False)\n",
            "    (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
            "    (output): LogSoftmax(dim=None)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzutKQ6UjK-p"
      },
      "source": [
        "## **5-TRAIN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVRN_BFFc2BY"
      },
      "source": [
        "epochs = 30\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Cuda available: \",torch.cuda.is_available())\n",
        "print(\"Current device: \",  torch.cuda.current_device())\n",
        "\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "max_f1 = 0\n",
        "\n",
        "losses = []\n",
        "for e in range(epochs):\n",
        "    print(f\"\\nepoch {e+1}/{epochs}\")\n",
        "    loss = train(model, train_loader)\n",
        "    print(f\" loss = {loss}\")\n",
        "    losses.append(loss)\n",
        "    val_loss, val_accuracy, val_f1, val_recall, val_precision = evaluate(model, val_loader)\n",
        "    # train_loss, train_accuracy, train_f1, _ = evaluate(model, train_loader)\n",
        "\n",
        "    \n",
        "    if val_f1 >= max_f1:\n",
        "        torch.save(model, save_model_path + 'best-model-max-f1.pt') \n",
        "        max_f1 = val_f1\n",
        "\n",
        "    # torch.save(model, save_model_path + 'model-e'+ str(e+1)+ '.pt')\n",
        "\n",
        "    print(\" \")\n",
        "    print(\"Validation loss: \", val_loss, \"/ Accuracy: \", val_accuracy, \" / f1: \", val_f1, \" / recall: \", val_recall, \" / precision: \", val_precision)\n",
        "    if e+1 == 6:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ven8WqtPojRD"
      },
      "source": [
        "## **6-Prepare and Predict Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO058JunooE5",
        "outputId": "5637273d-45ac-4f54-8744-59dd3def22f6"
      },
      "source": [
        "model = torch.load(\"/content/MZareiFolder/DL1_MZarei/Models/model6.pt\")  \n",
        "model.eval()\n",
        "\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "def image_loader(image_name):\n",
        "    \"\"\"load image, returns cuda tensor\"\"\"\n",
        "    image = cv2.imread(image_name)\n",
        "    image = Image.fromarray(image)\n",
        "\n",
        "    image = loader(image).float()\n",
        "    image = Variable(image, requires_grad=True)\n",
        "    image = image.unsqueeze(0)  \n",
        "    return image.cuda()  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_image_names = []\n",
        "test_labels_pred = []\n",
        "pred_prob = []\n",
        "test_Paths = list(paths.list_images(\"MZareiFolder/test/test\"))\n",
        "\n",
        "\n",
        "for test_Path in tqdm(test_Paths):\n",
        "    image_name = test_Path.split(os.path.sep)[-1]\n",
        "\n",
        "    # load image and change color channels\n",
        "    image = image_loader(test_Path) \n",
        "\n",
        "    output = model(image)\n",
        "    prd = output.argmax(1)\n",
        "\n",
        "    test_image_names.append(image_name)\n",
        "    test_labels_pred.append(prd.item())\n",
        "\n",
        "\n",
        "test_image_names = np.array(test_image_names)\n",
        "test_labels_pred = np.array(test_labels_pred)\n",
        "\n",
        "\n",
        "output = pd.DataFrame()\n",
        "output['File'] = test_image_names\n",
        "output['Label'] = test_labels_pred\n",
        "output.to_csv('test_pred_labels.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/400 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "100%|██████████| 400/400 [00:03<00:00, 115.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GtyBd_nNnGO"
      },
      "source": [
        "## **Bagging for Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50H2FWqqrjM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3ca2bb-1d2b-487a-e35e-5889554f533a"
      },
      "source": [
        "\n",
        "PATH_TO_MODELS = '/content/MZareiFolder/DL1_MZarei/Models'\n",
        "val_loader=torch.utils.data.DataLoader(ValidationData, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def most_frequent(List): \n",
        "    occurence_count = Counter(List) \n",
        "    return occurence_count.most_common(1)[0][0] \n",
        "\n",
        "\n",
        "\n",
        "def evaluate_bagging(models_paths, dataloader):\n",
        "\n",
        "    correct = 0\n",
        "    test_loss = 0\n",
        "    pred_list = np.array([])\n",
        "    label_list = np.array([])\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in tqdm(enumerate(dataloader)):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            prd_all_model = []\n",
        "            for model_path in models_paths:\n",
        "                model = torch.load(model_path)\n",
        "                model.eval()\n",
        "                output = model(images)\n",
        "                output = output.argmax(1)\n",
        "                prd_all_model.append(output.cpu().numpy()[0])\n",
        "\n",
        "            \n",
        "            prd = most_frequent(prd_all_model)\n",
        "\n",
        "            label_list= np.append(label_list, labels.cpu().numpy())\n",
        "            pred_list = np.append(pred_list, prd)\n",
        "\n",
        "            correct += torch.sum(labels == prd)\n",
        "    \n",
        "    accuracy = accuracy_score(label_list, pred_list)\n",
        "    f1 = f1_score(label_list, pred_list)\n",
        "    recall = recall_score(label_list, pred_list)\n",
        "    precision = precision_score(label_list, pred_list)\n",
        "    return accuracy, f1, recall, precision, label_list, pred_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Getting path to combo models\n",
        "models_paths_ = list(paths.list_files(PATH_TO_MODELS))\n",
        "combo = ['model'+item+'.pt' for item in input(\"Enter the list model e.g.  1 2 3  : \").split()]\n",
        "print(combo)\n",
        "models_paths = []\n",
        "for path in models_paths_:\n",
        "    model_name = path.split(os.path.sep)[-1]\n",
        "    if model_name in combo:\n",
        "        models_paths.append(path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "accuracy, f1, recall, precision, label_list, pred_list = evaluate_bagging(models_paths, val_loader)\n",
        "print(accuracy, f1, recall, precision)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the list model e.g.  A B C  : K I J H G F L M N O\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['modelK.pt', 'modelI.pt', 'modelJ.pt', 'modelH.pt', 'modelG.pt', 'modelF.pt', 'modelL.pt', 'modelM.pt', 'modelN.pt', 'modelO.pt']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "1527it [08:16,  3.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9927963326784545 0.964401294498382 0.9490445859872612 0.9802631578947368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpLxsp0AugNf"
      },
      "source": [
        "## **Bagging fot Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yWdukZ2VnLM",
        "outputId": "e60a9c58-0059-4387-eab5-4ab372441181"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "PATH_TO_MODELS = '/content/MZareiFolder/DL1_MZarei/Models'\n",
        "\n",
        "## Getting paths to combo models\n",
        "models_paths_ = list(paths.list_files(PATH_TO_MODELS))\n",
        "combo = ['model'+item+'.pt' for item in input(\"Enter the list model e.g. 1 2 3 : \").split()]\n",
        "models_paths = []\n",
        "for path in models_paths_:\n",
        "    model_name = path.split(os.path.sep)[-1]\n",
        "    if model_name in combo:\n",
        "        models_paths.append(path)\n",
        "\n",
        "\n",
        "test_Paths = list(paths.list_images(\"MZareiFolder/test/test\"))\n",
        "print(models_paths)\n",
        "\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "\n",
        "def most_frequent(List): \n",
        "    occurence_count = Counter(List) \n",
        "    return occurence_count.most_common(1)[0][0] \n",
        "\n",
        "def image_loader(image_name):\n",
        "    \"\"\"load image, returns cuda tensor\"\"\"\n",
        "    image = cv2.imread(image_name)\n",
        "    image = Image.fromarray(image)\n",
        "\n",
        "    image = loader(image).float()\n",
        "    image = Variable(image, requires_grad=True)\n",
        "    image = image.unsqueeze(0)  \n",
        "    return image.cuda()  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_image_names = []\n",
        "test_labels_pred = []\n",
        "pred_prob = []\n",
        "for test_Path in tqdm(test_Paths):\n",
        "    image_name = test_Path.split(os.path.sep)[-1]\n",
        "\n",
        "    # load image and change color channels\n",
        "    image = image_loader(test_Path)\n",
        "\n",
        "    \n",
        "    prd_all_model = []\n",
        "    for model_path in models_paths:\n",
        "        model = torch.load(model_path)\n",
        "        model.eval()\n",
        "        output = model(image)\n",
        "        output = output.argmax(1)\n",
        "        prd_all_model.append(output.cpu().numpy()[0])\n",
        "\n",
        "    prd = most_frequent(prd_all_model)\n",
        "\n",
        "    test_image_names.append(image_name)\n",
        "    test_labels_pred.append(prd)\n",
        "\n",
        "\n",
        "test_image_names = np.array(test_image_names)\n",
        "test_labels_pred = np.array(test_labels_pred)\n",
        "\n",
        "\n",
        "output = pd.DataFrame()\n",
        "output['File'] = test_image_names\n",
        "output['Label'] = test_labels_pred\n",
        "output.to_csv('test_pred_labels_ABC.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the list model e.g. A B C : K I J H G F L M N O\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/Models/modelG.pt', '/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/Models/modelH.pt', '/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/Models/modelF.pt', '/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/Models/modelJ.pt', '/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/Models/modelI.pt', '/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/Models/modelK.pt', '/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/Models/modelL.pt', '/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/Models/modelM.pt', '/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/Models/modelN.pt', '/content/drive/MyDrive/Colab Notebooks/COVID_DataChallenge1/Models/modelO.pt']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "100%|██████████| 400/400 [02:07<00:00,  3.13it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD71p-rAWQqS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}